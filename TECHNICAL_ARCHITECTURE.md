# ğŸ—ï¸ Technical Architecture Document

## System Design - Local RAG System

**Version**: 1.0.0  
**Date**: January 8, 2026  
**Architecture**: Production-Grade RAG Pipeline  

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Component Architecture](#component-architecture)
3. [Data Flow](#data-flow)
4. [Technology Stack](#technology-stack)
5. [Design Patterns](#design-patterns)
6. [Security Architecture](#security-architecture)
7. [Performance Optimization](#performance-optimization)
8. [Deployment Architecture](#deployment-architecture)

---

## 1. System Overview

### 1.1 Architecture Style
**Monolithic with Pipeline Architecture**
- Single Python application
- Event-driven document processing
- RESTful API layer
- File-based persistence

### 1.2 Core Principles
1. **Offline-First**: No external dependencies
2. **Fail-Safe**: Comprehensive validation before execution
3. **Observable**: Extensive logging and health checks
4. **Maintainable**: Clear separation of concerns
5. **Scalable**: Efficient vector operations for large datasets

---

## 2. Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Presentation Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Frontend (index.html)                                   â”‚   â”‚
â”‚  â”‚  â€¢ Glassmorphism UI                                      â”‚   â”‚
â”‚  â”‚  â€¢ Vanilla JavaScript                                    â”‚   â”‚
â”‚  â”‚  â€¢ Fetch API for backend calls                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†• HTTP/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Layer                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  FastAPI Server (main.py)                               â”‚   â”‚
â”‚  â”‚  â€¢ POST /api/ask - Question answering                   â”‚   â”‚
â”‚  â”‚  â€¢ GET /api/health - Health checks                      â”‚   â”‚
â”‚  â”‚  â€¢ GET /api/stats - System statistics                   â”‚   â”‚
â”‚  â”‚  â€¢ GET / - Serve frontend                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Business Logic Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RAG Backend (rag_backend.py)                           â”‚   â”‚
â”‚  â”‚  â€¢ Query embedding                                       â”‚   â”‚
â”‚  â”‚  â€¢ Vector similarity search                              â”‚   â”‚
â”‚  â”‚  â€¢ Context preparation                                   â”‚   â”‚
â”‚  â”‚  â€¢ LLM orchestration                                     â”‚   â”‚
â”‚  â”‚  â€¢ Hot-reload management                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Layer                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  LanceDB             â”‚         â”‚  LM Studio           â”‚     â”‚
â”‚  â”‚  â€¢ Vector storage    â”‚         â”‚  â€¢ LLM inference     â”‚     â”‚
â”‚  â”‚  â€¢ Similarity search â”‚         â”‚  â€¢ OpenAI API compat â”‚     â”‚
â”‚  â”‚  â€¢ File-based        â”‚         â”‚  â€¢ localhost:1234    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Processing Pipeline                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Document Watcher (watch_documents.py)                  â”‚   â”‚
â”‚  â”‚  â†“                                                       â”‚   â”‚
â”‚  â”‚  Phase 1: Text Extraction (phase1_extract.py)          â”‚   â”‚
â”‚  â”‚  â†“                                                       â”‚   â”‚
â”‚  â”‚  Phase 2: Embedding (phase2_embed.py)                  â”‚   â”‚
â”‚  â”‚  â†“                                                       â”‚   â”‚
â”‚  â”‚  Hot-Reload Trigger                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Data Flow

### 3.1 Document Processing Flow

```
PDF File
  â†“
[File Watcher Detects]
  â†“
[Phase 1: Text Extraction]
  â”œâ”€ PyMuPDF reads PDF page-by-page
  â”œâ”€ Extract text + metadata
  â”œâ”€ Output: JSONL (page, text, filename)
  â†“
[Phase 2: Embedding]
  â”œâ”€ Load JSONL file
  â”œâ”€ Chunk text (500 chars, 50 overlap)
  â”œâ”€ BGE model creates embeddings
  â”œâ”€ Store vectors in LanceDB
  â†“
[Trigger Hot-Reload]
  â”œâ”€ Create .reload_trigger marker
  â†“
[Backend Detects Trigger]
  â”œâ”€ Reload database
  â”œâ”€ Delete trigger marker
  â†“
[Document Ready for Queries]
```

### 3.2 Query Processing Flow

```
User Question
  â†“
[Frontend Submission]
  â”œâ”€ POST /api/ask with question
  â†“
[API Layer]
  â”œâ”€ Validate request
  â”œâ”€ Check for hot-reload trigger
  â†“
[RAG Backend]
  â”œâ”€ Embed question with BGE
  â”œâ”€ Vector similarity search (LanceDB)
  â”œâ”€ Retrieve top-K chunks
  â”œâ”€ Filter by relevance threshold
  â”œâ”€ Prepare context string
  â”œâ”€ Build system + user prompts
  â†“
[LM Studio]
  â”œâ”€ Send prompt to LLM
  â”œâ”€ Generate answer
  â”œâ”€ Return completion
  â†“
[Response Processing]
  â”œâ”€ Extract answer text
  â”œâ”€ Attach source metadata
  â”œâ”€ Calculate metrics
  â†“
[Frontend Display]
  â”œâ”€ Render answer
  â”œâ”€ Show source cards
  â”œâ”€ Display confidence scores
```

### 3.3 Hot-Reload Mechanism

```
[Document Processor]
  â”œâ”€ Process document
  â”œâ”€ Update database
  â”œâ”€ Create "data/.reload_trigger" file
  
[RAG Backend - On Every Request]
  â”œâ”€ Check if ".reload_trigger" exists
  â”œâ”€ If yes:
  â”‚   â”œâ”€ Reload LanceDB connection
  â”‚   â”œâ”€ Refresh chunk count
  â”‚   â”œâ”€ Delete ".reload_trigger"
  â”œâ”€ Continue with request
```

---

## 4. Technology Stack

### 4.1 Core Technologies

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **Web Framework** | FastAPI | 0.104+ | Async REST API |
| **Vector DB** | LanceDB | 0.3+ | Vector storage & search |
| **Embeddings** | BGE (sentence-transformers) | Latest | Query & document embedding |
| **LLM** | LM Studio | Latest | Local inference |
| **PDF Processing** | PyMuPDF (fitz) | 1.23+ | Text extraction |
| **File Watching** | watchdog | 3.0+ | File system monitoring |
| **HTTP Client** | httpx | 0.25+ | Async HTTP for LLM calls |

### 4.2 Python Dependencies

```txt
# Core Framework
fastapi==0.104.1
uvicorn[standard]==0.24.0

# Vector Database
lancedb==0.3.3
pyarrow==14.0.1

# ML & Embeddings
sentence-transformers==2.2.2
torch==2.1.0

# Document Processing
PyMuPDF==1.23.8
python-multipart==0.0.6

# HTTP & API
httpx==0.25.1
openai==1.3.0

# File Watching
watchdog==3.0.0

# Utilities
python-dotenv==1.0.0
jinja2==3.1.2
```

### 4.3 Hardware Optimization

**Apple Silicon (M-Series)**:
```python
# Automatic MPS detection
if torch.backends.mps.is_available():
    device = torch.device("mps")
# Uses Metal Performance Shaders for GPU acceleration
```

**Benefits**:
- 5-10x faster embeddings vs CPU
- Lower power consumption
- Efficient memory usage with unified memory

---

## 5. Design Patterns

### 5.1 Singleton Pattern
**RAG Backend Instance**
```python
# In main.py
rag_backend = RAGBackend()  # Single instance

@app.on_event("startup")
async def startup():
    await rag_backend.initialize()
```

**Why**: Share model loading and database connections

### 5.2 Factory Pattern
**Document Processor Factory**
```python
def get_processor(file_type: str):
    if file_type == '.pdf':
        return PDFProcessor()
    elif file_type == '.txt':
        return TextProcessor()
    # ...
```

**Why**: Extensible to new document types

### 5.3 Observer Pattern
**File Watcher**
```python
class DocumentHandler(FileSystemEventHandler):
    def on_created(self, event):
        self.process_document(event.src_path)
```

**Why**: React to file system events

### 5.4 Strategy Pattern
**Retrieval Strategy**
```python
if USE_RERANKER:
    results = rerank_strategy(query, candidates)
else:
    results = vector_only_strategy(query)
```

**Why**: Switch retrieval algorithms

### 5.5 Template Method Pattern
**Processing Pipeline**
```python
def process_document(file_path):
    extract_text()      # Phase 1
    create_embeddings() # Phase 2
    trigger_reload()    # Finalize
```

**Why**: Fixed pipeline structure, customizable steps

---

## 6. Security Architecture

### 6.1 Threat Model

| Threat | Mitigation | Status |
|--------|-----------|--------|
| **XSS Attacks** | HTML escaping in frontend | âœ… Implemented |
| **Path Traversal** | Restrict file access to documents/ | âœ… Implemented |
| **Code Injection** | No eval(), safe JSON parsing | âœ… Safe |
| **Data Exfiltration** | No external network calls | âœ… Offline only |
| **Prompt Injection** | LLM given context-only prompts | âš ï¸ Inherent risk |

### 6.2 Input Validation

**Frontend**:
```javascript
function escapeHtml(text) {
    const map = {
        '&': '&amp;',
        '<': '&lt;',
        '>': '&gt;',
        '"': '&quot;',
        "'": '&#039;'
    };
    return text.replace(/[&<>"']/g, m => map[m]);
}
```

**Backend**:
```python
class QuestionRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=1000)
    include_sources: bool = True
    file_filter: Optional[str] = None
```

### 6.3 File System Security

**Restricted Paths**:
```python
ALLOWED_DIRS = ['documents', 'documents/extracted_text']

def is_safe_path(path: Path) -> bool:
    return any(str(path).startswith(d) for d in ALLOWED_DIRS)
```

---

## 7. Performance Optimization

### 7.1 Database Optimization

**LanceDB Configuration**:
```python
# Approximate search for speed
table = db.open_table("docs")
results = table.search(query_vector).limit(10).to_list()
```

**Index Strategy**:
- IVF (Inverted File Index) for large datasets
- Automatic index creation on first query
- Trade-off: 1-2% accuracy loss for 10x speed

### 7.2 Embedding Optimization

**Batch Processing**:
```python
# Process multiple chunks at once
embeddings = model.encode(
    chunks,
    batch_size=32,
    show_progress_bar=True,
    device='mps'  # GPU acceleration
)
```

**Caching Strategy**:
```python
# Model loaded once, reused for all queries
@lru_cache(maxsize=1)
def get_embedding_model():
    return SentenceTransformer('BAAI/bge-large-en-v1.5')
```

### 7.3 Memory Management

**Streaming PDF Processing**:
```python
# Process page-by-page, not entire PDF at once
for page_num in range(doc.page_count):
    page = doc.load_page(page_num)
    text = page.get_text()
    yield {"page": page_num, "text": text}
```

**Benefits**:
- Handle 77MB+ PDFs without crashes
- Constant memory usage regardless of file size

### 7.4 Network Optimization

**Connection Pooling**:
```python
# Reuse HTTP connections to LM Studio
client = httpx.AsyncClient(
    base_url="http://localhost:1234/v1",
    timeout=60.0,
    limits=httpx.Limits(max_connections=10)
)
```

---

## 8. Deployment Architecture

### 8.1 Local Development

```
Developer Machine
â”œâ”€â”€ Python Virtual Environment (.venv)
â”œâ”€â”€ LM Studio (localhost:1234)
â”œâ”€â”€ FastAPI Server (localhost:8000)
â”œâ”€â”€ Vector Database (./data/index/)
â””â”€â”€ Documents (./documents/)
```

**Startup**:
```bash
./start_everything.sh
# Runs all pre-flight checks
# Starts server in background
```

### 8.2 Single-User Production

**Requirements**:
- macOS/Linux/Windows
- 16GB+ RAM
- 10GB+ storage
- Python 3.9+
- LM Studio installed

**Deployment**:
```bash
# 1. Clone repository
git clone <repo>

# 2. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 3. Start LM Studio with model

# 4. Launch system
./start_everything.sh
```

### 8.3 Multi-User Setup (Future)

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nginx      â”‚ (Reverse proxy)
â”‚  :80        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚ (x3 workers)
â”‚  :8000-8002 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚  LanceDB    â”‚ (Shared database)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Considerations**:
- Add authentication (JWT)
- Implement rate limiting
- Queue system for long-running queries
- Redis for caching
- PostgreSQL for user management

### 8.4 Docker Deployment (Future)

**Dockerfile**:
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Run server
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.yml**:
```yaml
version: '3.8'
services:
  rag-backend:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./documents:/app/documents
    environment:
      - LANCEDB_PATH=/app/data/index
```

---

## 9. Monitoring & Observability

### 9.1 Logging Architecture

**Structured Logging**:
```python
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('server.log'),
        logging.StreamHandler()
    ]
)
```

**Log Files**:
- `server.log` - API requests, errors, query metrics
- `watcher.log` - Document processing events

### 9.2 Health Checks

**Endpoint**: `GET /api/health`

**Checks**:
1. Database connectivity
2. Embedding model loaded
3. LM Studio reachable
4. Disk space available

**Response**:
```json
{
  "status": "healthy",
  "backend_status": {
    "embedding_model": "BAAI/bge-large-en-v1.5",
    "database_connected": true,
    "lm_studio_url": "http://localhost:1234/v1"
  }
}
```

### 9.3 Metrics Collection

**Query Metrics**:
```python
logger.info(f"Query: {query[:50]}...")
logger.info(f"Retrieved {len(sources)} sources")
logger.info(f"Answer generated in {duration}s")
```

**Processing Metrics**:
```python
logger.info(f"Phase 1: {page_count} pages processed")
logger.info(f"Phase 2: {chunk_count} chunks embedded")
```

---

## 10. Error Handling Strategy

### 10.1 Error Categories

| Category | Handling | User Message |
|----------|----------|--------------|
| **Validation** | Return 400 | "Invalid input: {details}" |
| **Not Found** | Return 404 | "Resource not found" |
| **LLM Failure** | Return 503 | "LLM service unavailable" |
| **Database** | Return 500 | "Database error" |
| **Unexpected** | Return 500 | "Please try again" |

### 10.2 Error Propagation

```python
try:
    result = await rag_backend.answer(query)
except ValueError as e:
    # User error
    raise HTTPException(status_code=400, detail=str(e))
except httpx.ConnectError:
    # LM Studio down
    raise HTTPException(status_code=503, detail="LLM unavailable")
except Exception as e:
    # Unexpected
    logger.error(f"Unexpected error: {e}", exc_info=True)
    raise HTTPException(status_code=500, detail="Internal error")
```

### 10.3 Graceful Degradation

**LM Studio Unavailable**:
- Pre-flight check catches this
- Won't start server if LM Studio down
- User gets clear "Start LM Studio" message

**Database Missing**:
- System creates empty database
- Processing pipeline populates it
- User can query once documents processed

---

## 11. Testing Strategy

### 11.1 Test Pyramid

```
     /\
    /  \  E2E (Manual UAT)
   /â”€â”€â”€â”€\
  / Unit \ (test_retrieval.py)
 /â”€â”€â”€â”€â”€â”€â”€â”€\
/ Integration \ (verify_setup.py)
```

### 11.2 Test Coverage

**Unit Tests**:
- Embedding generation
- Chunking logic
- Similarity calculations

**Integration Tests**:
- Database operations
- LLM API calls
- Pipeline execution

**E2E Tests**:
- Full question-answer flow
- Document processing
- Hot-reload mechanism

### 11.3 Test Files

```python
# test_retrieval.py
def test_query_embedding():
    vector = embed_query("test query")
    assert len(vector) == 1024

# verify_setup.py
def verify_dependencies():
    check_python_version()
    check_packages_installed()
    check_lm_studio_running()
```

---

## 12. Configuration Management

### 12.1 Environment Variables

**Storage**: `.env` file (gitignored)

**Categories**:
1. **Database** - Paths and table names
2. **Models** - Embedding and LLM models
3. **Retrieval** - Search parameters
4. **Performance** - Batch sizes, timeouts
5. **Features** - Enable/disable reranker

### 12.2 Configuration Loading

```python
from dotenv import load_dotenv
import os

load_dotenv()

LANCEDB_PATH = os.getenv('LANCEDB_PATH', './data/index')
TOP_K = int(os.getenv('TOP_K_FINAL', '5'))
```

### 12.3 Validation

**Startup Validation**:
```python
def validate_config():
    required = ['LANCEDB_PATH', 'EMBEDDING_MODEL']
    for key in required:
        if key not in os.environ:
            raise ValueError(f"Missing config: {key}")
```

---

## 13. Scalability Considerations

### 13.1 Current Limits

| Resource | Current | Max Tested | Theoretical |
|----------|---------|------------|-------------|
| **Documents** | 4 | 10 | 1,000+ |
| **Chunks** | 9,199 | 50,000 | 1M+ |
| **Concurrent Users** | 1 | 1 | 10-20 |
| **Query Rate** | Ad-hoc | N/A | 10-20 QPS |

### 13.2 Bottlenecks

1. **LM Studio**: Single-threaded inference (blocking)
2. **Embedding**: GPU limited (MPS has queue)
3. **Database**: File-based (no network overhead)
4. **Memory**: Models loaded in RAM (2GB+)

### 13.3 Scaling Strategies

**Vertical Scaling**:
- More RAM â†’ Larger models
- Better GPU â†’ Faster embeddings
- Faster SSD â†’ Quicker database ops

**Horizontal Scaling**:
- Multiple FastAPI workers
- Load balancer (Nginx)
- Shared LanceDB (read-only replicas)
- Multiple LM Studio instances

---

## 14. Maintenance & Operations

### 14.1 Regular Tasks

**Daily**:
- Check `server.log` for errors
- Monitor disk space

**Weekly**:
- Review query patterns
- Update LM Studio models if needed

**Monthly**:
- Backup `data/index/` directory
- Archive old logs
- Update dependencies

### 14.2 Backup Strategy

**Critical Data**:
1. **Vector Database**: `data/index/` (~1GB)
2. **Original Documents**: `documents/` (varies)
3. **Configuration**: `.env` file

**Backup Command**:
```bash
tar -czf backup-$(date +%Y%m%d).tar.gz data/index/ documents/ .env
```

### 14.3 Disaster Recovery

**Scenarios**:

**Database Corruption**:
```bash
rm -rf data/index/
./auto_pipeline.sh  # Rebuild from documents
```

**Lost Documents**:
- Restore from backup
- Reprocess if backup unavailable

**Configuration Loss**:
- Copy from `.env.example`
- Adjust for local setup

---

## 15. Architecture Decisions (ADRs)

### ADR-001: Two-Phase Processing
**Decision**: Split extraction and embedding into separate phases  
**Rationale**: Memory efficiency, fault tolerance, reusability  
**Trade-offs**: More disk I/O, intermediate storage  

### ADR-002: LM Studio over Ollama
**Decision**: Use LM Studio for LLM inference  
**Rationale**: Better UI, model management, OpenAI compatibility  
**Trade-offs**: Less scriptable than Ollama  

### ADR-003: LanceDB over Chroma
**Decision**: Use LanceDB for vector storage  
**Rationale**: File-based, no server, fast, columnar format  
**Trade-offs**: Less mature ecosystem  

### ADR-004: No Hallucination Tolerance
**Decision**: Strict source-only responses  
**Rationale**: Trust and reliability > flexibility  
**Trade-offs**: Less conversational, more robotic  

### ADR-005: Hot-Reload via Marker File
**Decision**: Use `.reload_trigger` file for IPC  
**Rationale**: Simple, no shared memory, no sockets  
**Trade-offs**: Polling overhead (minimal)  

---

## 16. Future Architecture Evolution

### Phase 2: Multi-User Support
- Add authentication layer
- Implement user-specific document collections
- Per-user rate limiting
- Shared vector database with access control

### Phase 3: Cloud Deployment
- Containerize with Docker
- Deploy to AWS/GCP/Azure
- S3 for document storage
- RDS for user management
- ElastiCache for query caching

### Phase 4: Advanced Features
- Knowledge graph extraction
- Multi-modal support (images, tables)
- Conversation memory
- Fine-tuned embedding models

---

**Document Version**: 1.0  
**Last Updated**: January 8, 2026  
**Architecture Status**: Stable, Production-Ready  

ğŸ—ï¸ **Architecture Quality: Enterprise-Grade**
